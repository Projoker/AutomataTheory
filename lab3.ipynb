{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "564c7f1c",
   "metadata": {},
   "source": [
    "Задача 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "796129a9",
   "metadata": {},
   "source": [
    "Установить gym (python3 - https://github.com/openai/gym, для тех, кто делает на java - https://github.com/deeplearning4j/gym-java-client), реализовать среду из предыдущего семинара в gym, агенты - тигр и кролик. Реализовать задачу из семинара 2 в openai gym. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec30323d",
   "metadata": {},
   "source": [
    "Задача 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8652bdea",
   "metadata": {},
   "source": [
    "Реализовать поведение \"поиска\" добычи тигром - тигр исследует карту и выслеживает добычу (оказывается в 3 клетках от нее - добыча выслежена). Далее он следует к добыче и пытается ее поймать (как на 1 семинаре). Если кролик уворачивается от тигра, то он отбегает на 5 клеток в любом направлении. Каждый раз, после неудачной ловли, тигр усовершенствует свой уровень охотника на 10 %. Для обеспечения поиска добычи использовать DQN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nbakulin/PycharmProjects/labs/venv/lib/python3.9/site-packages/gym/utils/play.py:29: UserWarning: \u001B[33mWARN: Matplotlib is not installed, run `pip install gym[other]`\u001B[0m\n",
      "  logger.warn(\"Matplotlib is not installed, run `pip install gym[other]`\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M1 Pro\n",
      "Model: \"DQNetwork\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 24)                624       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 24)                600       \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 4)                 100       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,324\n",
      "Trainable params: 1,324\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nbakulin/PycharmProjects/labs/venv/lib/python3.9/site-packages/gym/utils/passive_env_checker.py:31: UserWarning: \u001B[33mWARN: A Box observation space has an unconventional shape (neither an image, nor a 1D vector). We recommend flattening the observation to have only a 1D vector or use a custom policy to properly process the data. Actual observation shape: (2, 2)\u001B[0m\n",
      "  logger.warn(\n",
      "2023-01-06 03:19:29.246093: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2023-01-06 03:19:29.246186: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n",
      "/Users/nbakulin/PycharmProjects/labs/venv/lib/python3.9/site-packages/gym/utils/passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n",
      "2023-01-06 03:19:29.351262: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n",
      "2023-01-06 03:19:29.381029: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2023-01-06 03:19:29.427687: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "Episode 1/1000, Reward 1983\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-06 03:19:29.612501: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2/1000, Reward -2036\n",
      "Episode 3/1000, Reward -2001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-06 03:19:31.979465: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 4/1000, Reward -1530\n",
      "Episode 5/1000, Reward -2028\n",
      "Episode 6/1000, Reward -2001\n",
      "Episode 7/1000, Reward 1986\n",
      "Episode 8/1000, Reward -1967\n",
      "Episode 9/1000, Reward -2175\n",
      "Episode 10/1000, Reward -2995\n",
      "Episode 11/1000, Reward -2579\n",
      "Episode 12/1000, Reward -3523\n",
      "Episode 13/1000, Reward -7168\n",
      "Episode 14/1000, Reward -5767\n",
      "Episode 15/1000, Reward 1819\n",
      "Episode 16/1000, Reward -1667\n",
      "Episode 17/1000, Reward -22029\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[1], line 123\u001B[0m\n\u001B[1;32m    120\u001B[0m dqn\u001B[38;5;241m.\u001B[39mreplay_memory\u001B[38;5;241m.\u001B[39mappend([observation, action, reward, new_observation, done])\n\u001B[1;32m    122\u001B[0m \u001B[38;5;66;03m# Update q_values\u001B[39;00m\n\u001B[0;32m--> 123\u001B[0m \u001B[43mdqn\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    125\u001B[0m \u001B[38;5;66;03m# Update state\u001B[39;00m\n\u001B[1;32m    126\u001B[0m observation \u001B[38;5;241m=\u001B[39m new_observation\n",
      "Cell \u001B[0;32mIn[1], line 52\u001B[0m, in \u001B[0;36mDQNetwork.train\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m     49\u001B[0m mini_batch \u001B[38;5;241m=\u001B[39m random\u001B[38;5;241m.\u001B[39msample(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mreplay_memory, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbatch_size)\n\u001B[1;32m     51\u001B[0m current_states \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39marray([transition[\u001B[38;5;241m0\u001B[39m] \u001B[38;5;28;01mfor\u001B[39;00m transition \u001B[38;5;129;01min\u001B[39;00m mini_batch])\n\u001B[0;32m---> 52\u001B[0m current_qs_list \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpredict\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcurrent_states\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mverbose\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m     54\u001B[0m new_current_states \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39marray([transition[\u001B[38;5;241m3\u001B[39m] \u001B[38;5;28;01mfor\u001B[39;00m transition \u001B[38;5;129;01min\u001B[39;00m mini_batch])\n\u001B[1;32m     55\u001B[0m future_qs_list \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtarget_model\u001B[38;5;241m.\u001B[39mpredict(new_current_states, verbose\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m)\n",
      "File \u001B[0;32m~/PycharmProjects/labs/venv/lib/python3.9/site-packages/keras/utils/traceback_utils.py:64\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     62\u001B[0m filtered_tb \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m     63\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 64\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     65\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:  \u001B[38;5;66;03m# pylint: disable=broad-except\u001B[39;00m\n\u001B[1;32m     66\u001B[0m   filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n",
      "File \u001B[0;32m~/PycharmProjects/labs/venv/lib/python3.9/site-packages/keras/engine/training.py:2002\u001B[0m, in \u001B[0;36mModel.predict\u001B[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001B[0m\n\u001B[1;32m   1995\u001B[0m   \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m:\n\u001B[1;32m   1996\u001B[0m     warnings\u001B[38;5;241m.\u001B[39mwarn(\n\u001B[1;32m   1997\u001B[0m         \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mUsing Model.predict with MultiWorkerMirroredStrategy or \u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[1;32m   1998\u001B[0m         \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mTPUStrategy and AutoShardPolicy.FILE might lead to out-of-order \u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[1;32m   1999\u001B[0m         \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mresult. Consider setting it to AutoShardPolicy.DATA.\u001B[39m\u001B[38;5;124m'\u001B[39m,\n\u001B[1;32m   2000\u001B[0m         stacklevel\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m2\u001B[39m)\n\u001B[0;32m-> 2002\u001B[0m data_handler \u001B[38;5;241m=\u001B[39m \u001B[43mdata_adapter\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_data_handler\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   2003\u001B[0m \u001B[43m    \u001B[49m\u001B[43mx\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2004\u001B[0m \u001B[43m    \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbatch_size\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2005\u001B[0m \u001B[43m    \u001B[49m\u001B[43msteps_per_epoch\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msteps\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2006\u001B[0m \u001B[43m    \u001B[49m\u001B[43minitial_epoch\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2007\u001B[0m \u001B[43m    \u001B[49m\u001B[43mepochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2008\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmax_queue_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmax_queue_size\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2009\u001B[0m \u001B[43m    \u001B[49m\u001B[43mworkers\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mworkers\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2010\u001B[0m \u001B[43m    \u001B[49m\u001B[43muse_multiprocessing\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43muse_multiprocessing\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2011\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2012\u001B[0m \u001B[43m    \u001B[49m\u001B[43msteps_per_execution\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_steps_per_execution\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   2014\u001B[0m \u001B[38;5;66;03m# Container that configures and calls `tf.keras.Callback`s.\u001B[39;00m\n\u001B[1;32m   2015\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(callbacks, callbacks_module\u001B[38;5;241m.\u001B[39mCallbackList):\n",
      "File \u001B[0;32m~/PycharmProjects/labs/venv/lib/python3.9/site-packages/keras/engine/data_adapter.py:1401\u001B[0m, in \u001B[0;36mget_data_handler\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m   1399\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mgetattr\u001B[39m(kwargs[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmodel\u001B[39m\u001B[38;5;124m\"\u001B[39m], \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_cluster_coordinator\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[1;32m   1400\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m _ClusterCoordinatorDataHandler(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m-> 1401\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mDataHandler\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/labs/venv/lib/python3.9/site-packages/keras/engine/data_adapter.py:1150\u001B[0m, in \u001B[0;36mDataHandler.__init__\u001B[0;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model, steps_per_execution, distribute)\u001B[0m\n\u001B[1;32m   1147\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   1148\u001B[0m   \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_steps_per_execution \u001B[38;5;241m=\u001B[39m steps_per_execution\n\u001B[0;32m-> 1150\u001B[0m adapter_cls \u001B[38;5;241m=\u001B[39m \u001B[43mselect_data_adapter\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1151\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_adapter \u001B[38;5;241m=\u001B[39m adapter_cls(\n\u001B[1;32m   1152\u001B[0m     x,\n\u001B[1;32m   1153\u001B[0m     y,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1162\u001B[0m     distribution_strategy\u001B[38;5;241m=\u001B[39mtf\u001B[38;5;241m.\u001B[39mdistribute\u001B[38;5;241m.\u001B[39mget_strategy(),\n\u001B[1;32m   1163\u001B[0m     model\u001B[38;5;241m=\u001B[39mmodel)\n\u001B[1;32m   1165\u001B[0m strategy \u001B[38;5;241m=\u001B[39m tf\u001B[38;5;241m.\u001B[39mdistribute\u001B[38;5;241m.\u001B[39mget_strategy()\n",
      "File \u001B[0;32m~/PycharmProjects/labs/venv/lib/python3.9/site-packages/keras/engine/data_adapter.py:982\u001B[0m, in \u001B[0;36mselect_data_adapter\u001B[0;34m(x, y)\u001B[0m\n\u001B[1;32m    980\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mselect_data_adapter\u001B[39m(x, y):\n\u001B[1;32m    981\u001B[0m   \u001B[38;5;124;03m\"\"\"Selects a data adapter than can handle a given x and y.\"\"\"\u001B[39;00m\n\u001B[0;32m--> 982\u001B[0m   adapter_cls \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28mcls\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m \u001B[38;5;28mcls\u001B[39m \u001B[38;5;129;01min\u001B[39;00m ALL_ADAPTER_CLS \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m.\u001B[39mcan_handle(x, y)]\n\u001B[1;32m    983\u001B[0m   \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m adapter_cls:\n\u001B[1;32m    984\u001B[0m     \u001B[38;5;66;03m# TODO(scottzhu): This should be a less implementation-specific error.\u001B[39;00m\n\u001B[1;32m    985\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m    986\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mFailed to find data adapter that can handle \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    987\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124minput: \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m, \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(\n\u001B[1;32m    988\u001B[0m             _type_name(x), _type_name(y)))\n",
      "File \u001B[0;32m~/PycharmProjects/labs/venv/lib/python3.9/site-packages/keras/engine/data_adapter.py:982\u001B[0m, in \u001B[0;36m<listcomp>\u001B[0;34m(.0)\u001B[0m\n\u001B[1;32m    980\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mselect_data_adapter\u001B[39m(x, y):\n\u001B[1;32m    981\u001B[0m   \u001B[38;5;124;03m\"\"\"Selects a data adapter than can handle a given x and y.\"\"\"\u001B[39;00m\n\u001B[0;32m--> 982\u001B[0m   adapter_cls \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28mcls\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m \u001B[38;5;28mcls\u001B[39m \u001B[38;5;129;01min\u001B[39;00m ALL_ADAPTER_CLS \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28;43mcls\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcan_handle\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my\u001B[49m\u001B[43m)\u001B[49m]\n\u001B[1;32m    983\u001B[0m   \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m adapter_cls:\n\u001B[1;32m    984\u001B[0m     \u001B[38;5;66;03m# TODO(scottzhu): This should be a less implementation-specific error.\u001B[39;00m\n\u001B[1;32m    985\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m    986\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mFailed to find data adapter that can handle \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    987\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124minput: \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m, \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(\n\u001B[1;32m    988\u001B[0m             _type_name(x), _type_name(y)))\n",
      "File \u001B[0;32m~/PycharmProjects/labs/venv/lib/python3.9/site-packages/keras/engine/data_adapter.py:564\u001B[0m, in \u001B[0;36mCompositeTensorDataAdapter.can_handle\u001B[0;34m(x, y)\u001B[0m\n\u001B[1;32m    561\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m    562\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m _is_composite(v)\n\u001B[0;32m--> 564\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m (\u001B[38;5;28;43many\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m_is_composite\u001B[49m\u001B[43m(\u001B[49m\u001B[43mv\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mv\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mflat_inputs\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;129;01mand\u001B[39;00m\n\u001B[1;32m    565\u001B[0m         \u001B[38;5;28mall\u001B[39m(_is_tensor_or_composite(v) \u001B[38;5;28;01mfor\u001B[39;00m v \u001B[38;5;129;01min\u001B[39;00m flat_inputs))\n",
      "File \u001B[0;32m~/PycharmProjects/labs/venv/lib/python3.9/site-packages/keras/engine/data_adapter.py:564\u001B[0m, in \u001B[0;36m<genexpr>\u001B[0;34m(.0)\u001B[0m\n\u001B[1;32m    561\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m    562\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m _is_composite(v)\n\u001B[0;32m--> 564\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m (\u001B[38;5;28many\u001B[39m(\u001B[43m_is_composite\u001B[49m\u001B[43m(\u001B[49m\u001B[43mv\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m v \u001B[38;5;129;01min\u001B[39;00m flat_inputs) \u001B[38;5;129;01mand\u001B[39;00m\n\u001B[1;32m    565\u001B[0m         \u001B[38;5;28mall\u001B[39m(_is_tensor_or_composite(v) \u001B[38;5;28;01mfor\u001B[39;00m v \u001B[38;5;129;01min\u001B[39;00m flat_inputs))\n",
      "File \u001B[0;32m~/PycharmProjects/labs/venv/lib/python3.9/site-packages/keras/engine/data_adapter.py:557\u001B[0m, in \u001B[0;36mCompositeTensorDataAdapter.can_handle.<locals>._is_composite\u001B[0;34m(v)\u001B[0m\n\u001B[1;32m    555\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m    556\u001B[0m \u001B[38;5;66;03m# Support Scipy sparse tensors if scipy is installed\u001B[39;00m\n\u001B[0;32m--> 557\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_is_scipy_sparse\u001B[49m\u001B[43m(\u001B[49m\u001B[43mv\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/labs/venv/lib/python3.9/site-packages/keras/engine/data_adapter.py:1667\u001B[0m, in \u001B[0;36m_is_scipy_sparse\u001B[0;34m(x)\u001B[0m\n\u001B[1;32m   1665\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_is_scipy_sparse\u001B[39m(x):\n\u001B[1;32m   1666\u001B[0m   \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m-> 1667\u001B[0m     \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mscipy\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msparse\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m issparse  \u001B[38;5;66;03m# pylint: disable=g-import-not-at-top\u001B[39;00m\n\u001B[1;32m   1669\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m issparse(x)\n\u001B[1;32m   1670\u001B[0m   \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mImportError\u001B[39;00m:\n",
      "File \u001B[0;32m<frozen importlib._bootstrap>:1007\u001B[0m, in \u001B[0;36m_find_and_load\u001B[0;34m(name, import_)\u001B[0m\n",
      "File \u001B[0;32m<frozen importlib._bootstrap>:982\u001B[0m, in \u001B[0;36m_find_and_load_unlocked\u001B[0;34m(name, import_)\u001B[0m\n",
      "File \u001B[0;32m<frozen importlib._bootstrap>:925\u001B[0m, in \u001B[0;36m_find_spec\u001B[0;34m(name, path, target)\u001B[0m\n",
      "File \u001B[0;32m~/PycharmProjects/labs/venv/lib/python3.9/site-packages/_distutils_hack/__init__.py:82\u001B[0m, in \u001B[0;36mDistutilsMetaFinder.find_spec\u001B[0;34m(self, fullname, path, target)\u001B[0m\n\u001B[1;32m     79\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m\n\u001B[1;32m     81\u001B[0m method_name \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mspec_for_\u001B[39m\u001B[38;5;132;01m{fullname}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m.\u001B[39mformat(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m\u001B[38;5;28mlocals\u001B[39m())\n\u001B[0;32m---> 82\u001B[0m method \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mgetattr\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmethod_name\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mlambda\u001B[39;49;00m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[1;32m     83\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m method()\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "import random\n",
    "from collections import deque\n",
    "import numpy as np\n",
    "import gym\n",
    "import my_gym\n",
    "from my_gym.wrappers import FlattenGridObservation\n",
    "from gym.utils.play import play\n",
    "import pygame\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras import layers, Sequential, losses, optimizers, models\n",
    "\n",
    "\n",
    "class DQNetwork:\n",
    "    def __init__(self, observation_shape, action_size, name='DQNetwork'):\n",
    "        self.exploration_rate = 1.0\n",
    "        self.exploration_rate_min = 0.01\n",
    "        self.exploration_rate_decay = 0.995\n",
    "        self.learning_rate = 0.001\n",
    "        self.gamma = 0.7\n",
    "        self.batch_size = 32\n",
    "\n",
    "        self.replay_memory = deque(maxlen=50_000)\n",
    "\n",
    "        self.model = Sequential([\n",
    "            layers.Input(shape=observation_shape),\n",
    "            layers.Dense(24, activation='relu'),\n",
    "            layers.Dense(24, activation='relu'),\n",
    "            layers.Dense(action_size, activation='linear'),\n",
    "        ], name)\n",
    "\n",
    "        self.model.compile(\n",
    "            optimizer=optimizers.Adam(self.learning_rate),\n",
    "            loss=losses.MeanSquaredError(),\n",
    "            metrics=['accuracy'],\n",
    "        )\n",
    "        print(self.model.summary())\n",
    "\n",
    "        self.target_model = models.clone_model(self.model)\n",
    "        self.update()\n",
    "\n",
    "    def update(self):\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "\n",
    "    def train(self):\n",
    "        if len(self.replay_memory) < self.batch_size:\n",
    "            return\n",
    "\n",
    "        mini_batch = random.sample(self.replay_memory, self.batch_size)\n",
    "\n",
    "        current_states = np.array([transition[0] for transition in mini_batch])\n",
    "        current_qs_list = self.model.predict(current_states, verbose=0)\n",
    "\n",
    "        new_current_states = np.array([transition[3] for transition in mini_batch])\n",
    "        future_qs_list = self.target_model.predict(new_current_states, verbose=0)\n",
    "\n",
    "        states = []\n",
    "        target_qs = []\n",
    "        for index, (observation, action, reward, new_observation, done) in enumerate(mini_batch):\n",
    "            current_qs = current_qs_list[index]\n",
    "\n",
    "            if done:\n",
    "                current_qs[action] = reward\n",
    "            else:\n",
    "                current_qs[action] = reward + self.gamma * np.max(future_qs_list[index])\n",
    "\n",
    "            states.append(observation)\n",
    "            target_qs.append(current_qs)\n",
    "\n",
    "        self.model.fit(np.array(states), np.array(target_qs), batch_size=self.batch_size, verbose=0, shuffle=True)\n",
    "\n",
    "        if self.exploration_rate > self.exploration_rate_min:\n",
    "            self.exploration_rate *= self.exploration_rate_decay\n",
    "\n",
    "    def get_qs(self, observation):\n",
    "        return self.model.predict(observation.reshape([1, observation.shape[0]]), verbose=0)[0]\n",
    "\n",
    "    def egreedy_policy(self, observation, action_size):\n",
    "        if np.random.random() < self.exploration_rate:\n",
    "            return np.random.choice(action_size)\n",
    "        else:\n",
    "            return np.argmax(self.get_qs(observation))\n",
    "\n",
    "\n",
    "# Create env\n",
    "base_env = gym.make('GridWorld-v0', size=5)\n",
    "env = FlattenGridObservation(base_env)\n",
    "env.action_space.seed(42)\n",
    "\n",
    "# Or play env\n",
    "# mapping = {(pygame.K_RIGHT,): 0, (pygame.K_DOWN,): 1, (pygame.K_LEFT,): 2, (pygame.K_UP,): 3}\n",
    "# play(gym.make('GridWorld-v0', render_mode=\"rgb_array\", size=5), keys_to_action=mapping, noop=4)\n",
    "\n",
    "# Parameters\n",
    "episodes = 1000\n",
    "\n",
    "# Instantiate the DQNetwork\n",
    "dqn = DQNetwork(env.observation_space.shape, env.action_space.n)\n",
    "\n",
    "for episode in range(episodes):\n",
    "    observation, info = env.reset()\n",
    "    done = False\n",
    "    reward_sum = 0\n",
    "    steps = 0\n",
    "\n",
    "    while not done:\n",
    "        steps += 1\n",
    "        # if steps == 500:\n",
    "        #     break\n",
    "\n",
    "        # Choose action\n",
    "        action = dqn.egreedy_policy(observation, env.action_space.n)\n",
    "\n",
    "        # Do the action\n",
    "        new_observation, reward, terminated, truncated, info = env.step(action)\n",
    "        reward_sum += reward\n",
    "        done = terminated or truncated\n",
    "\n",
    "        # Save in replay memory\n",
    "        dqn.replay_memory.append([observation, action, reward, new_observation, done])\n",
    "\n",
    "        # Update q_values\n",
    "        dqn.train()\n",
    "\n",
    "        # Update state\n",
    "        observation = new_observation\n",
    "\n",
    "    dqn.update()\n",
    "\n",
    "    print(f\"Episode {episode + 1}/{episodes}, Reward {reward_sum}\")\n",
    "\n",
    "env.close()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[2], line 11\u001B[0m\n\u001B[1;32m      9\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m done:\n\u001B[1;32m     10\u001B[0m     action \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39margmax(dqn\u001B[38;5;241m.\u001B[39mget_qs(observation))\n\u001B[0;32m---> 11\u001B[0m     new_observation, reward, terminated, truncated, info \u001B[38;5;241m=\u001B[39m \u001B[43menv\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstep\u001B[49m\u001B[43m(\u001B[49m\u001B[43maction\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     12\u001B[0m     done \u001B[38;5;241m=\u001B[39m terminated \u001B[38;5;129;01mor\u001B[39;00m truncated\n\u001B[1;32m     13\u001B[0m     observation \u001B[38;5;241m=\u001B[39m new_observation\n",
      "File \u001B[0;32m~/PycharmProjects/labs/venv/lib/python3.9/site-packages/gym/core.py:384\u001B[0m, in \u001B[0;36mObservationWrapper.step\u001B[0;34m(self, action)\u001B[0m\n\u001B[1;32m    382\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mstep\u001B[39m(\u001B[38;5;28mself\u001B[39m, action):\n\u001B[1;32m    383\u001B[0m     \u001B[38;5;124;03m\"\"\"Returns a modified observation using :meth:`self.observation` after calling :meth:`env.step`.\"\"\"\u001B[39;00m\n\u001B[0;32m--> 384\u001B[0m     observation, reward, terminated, truncated, info \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43menv\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstep\u001B[49m\u001B[43m(\u001B[49m\u001B[43maction\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    385\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mobservation(observation), reward, terminated, truncated, info\n",
      "File \u001B[0;32m~/PycharmProjects/labs/venv/lib/python3.9/site-packages/gym/wrappers/order_enforcing.py:37\u001B[0m, in \u001B[0;36mOrderEnforcing.step\u001B[0;34m(self, action)\u001B[0m\n\u001B[1;32m     35\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_has_reset:\n\u001B[1;32m     36\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m ResetNeeded(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCannot call env.step() before calling env.reset()\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m---> 37\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43menv\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstep\u001B[49m\u001B[43m(\u001B[49m\u001B[43maction\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/labs/venv/lib/python3.9/site-packages/gym/wrappers/env_checker.py:39\u001B[0m, in \u001B[0;36mPassiveEnvChecker.step\u001B[0;34m(self, action)\u001B[0m\n\u001B[1;32m     37\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m env_step_passive_checker(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39menv, action)\n\u001B[1;32m     38\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m---> 39\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43menv\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstep\u001B[49m\u001B[43m(\u001B[49m\u001B[43maction\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/labs/my_gym/envs/grid_world.py:127\u001B[0m, in \u001B[0;36mGridWorldEnv.step\u001B[0;34m(self, action)\u001B[0m\n\u001B[1;32m    124\u001B[0m info \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_get_info()\n\u001B[1;32m    126\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrender_mode \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhuman\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[0;32m--> 127\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_render_frame\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    129\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m observation, reward, terminated, truncated, info\n",
      "File \u001B[0;32m~/PycharmProjects/labs/my_gym/envs/grid_world.py:199\u001B[0m, in \u001B[0;36mGridWorldEnv._render_frame\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    196\u001B[0m     pygame\u001B[38;5;241m.\u001B[39mdisplay\u001B[38;5;241m.\u001B[39mupdate()\n\u001B[1;32m    198\u001B[0m     \u001B[38;5;66;03m# Add a delay to keep the framerate stable.\u001B[39;00m\n\u001B[0;32m--> 199\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mclock\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtick\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmetadata\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mrender_fps\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    200\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:  \u001B[38;5;66;03m# rgb_array\u001B[39;00m\n\u001B[1;32m    201\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m np\u001B[38;5;241m.\u001B[39mtranspose(\n\u001B[1;32m    202\u001B[0m         np\u001B[38;5;241m.\u001B[39marray(pygame\u001B[38;5;241m.\u001B[39msurfarray\u001B[38;5;241m.\u001B[39mpixels3d(canvas)), axes\u001B[38;5;241m=\u001B[39m(\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m2\u001B[39m)\n\u001B[1;32m    203\u001B[0m     )\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "base_env = gym.make('GridWorld-v0', size=5, render_mode=\"human\")\n",
    "env = FlattenGridObservation(base_env)\n",
    "env.action_space.seed(42)\n",
    "\n",
    "for episode in range(3):\n",
    "    observation, info = env.reset()\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        action = np.argmax(dqn.get_qs(observation))\n",
    "        new_observation, reward, terminated, truncated, info = env.step(action)\n",
    "        done = terminated or truncated\n",
    "        observation = new_observation\n",
    "\n",
    "env.close()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
